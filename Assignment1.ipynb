{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some basic library\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This classification metric based on probabilities.\n",
    "\n",
    "It measures the performance of a classification model where the prediction result into a probability value between  one / zero. \n",
    "Log loss increases as the predicted probability diverge from the actual label. \n",
    "The goal of  machine learning model to minimize this value. \n",
    "As such, smaller log loss better will the model,a perfect model having a log loss of zero.\n",
    "\n",
    "Implementation part use:\n",
    "    accuracy = log_loss(y_test, pred)\n",
    "    #print(\"Logloss: %.2f\" % (accuracy))\n",
    "\n",
    "Lower the value ,better will be the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Jaccard Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let’s understandan example.\n",
    "labeled test , labels    as –y =  [0,0,0,0,0,1,1,1,1,1]\n",
    "our model has predicted  as –y1 = [1,1,0,0,0,1,1,1,1,1]\n",
    "\n",
    "Jaccard Index / Jaccard similarity coefficient is a statistic used in understanding the similarities between sample sets.\n",
    "The measurement emphasizes the similarity between finite sample sets and  formally defined as the size of the intersection divided by the size of the union of the two labeled sets\n",
    "\n",
    "we can see that the intersection of the two sets is equal to 8 (since eight values are predicted correctly) and the union is 10 + 10–8 = 12. \n",
    "So, the Jaccard index gives the accuracy –: 66 %(8/12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Gini Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Gini coefficient/Gini Index is a popular metric for imbalanced class values. \n",
    "The coefficient ranges from 0 to 1 where 0 represents perfect equality and 1 represents perfect inequality. \n",
    "Here, if the value of an index is higher, then the data will be more dispersed.\n",
    "\n",
    "Gini coefficient can be computed from the area under the ROC curve using the following formula:\n",
    "\n",
    "Gini Coefficient = (2 * ROC_curve) — 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "df=pd.read_csv(\"dataset - Sheet1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Class</th>\n",
       "      <th>Actual Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted Class  Actual Class\n",
       "0                0             0\n",
       "1                0             0\n",
       "2                1             0\n",
       "3                0             0\n",
       "4                0             0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 160 entries, 0 to 159\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype\n",
      "---  ------           --------------  -----\n",
      " 0   Predicted Class  160 non-null    int64\n",
      " 1   Actual Class     160 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 2.6 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the number of multiclass\n",
    "a=df['Predicted Class'].unique()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true positive\n",
    "tp=df[df['Actual Class'] == df['Predicted Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Class</th>\n",
       "      <th>Actual Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted Class  Actual Class\n",
       "0                0             0\n",
       "1                0             0\n",
       "3                0             0\n",
       "4                0             0\n",
       "8                0             0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Therefore total number of correct prediction is 121 out of 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy\n",
    "Accuracy=121/160\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#true positive for class 0\n",
    "tp0=df[(df['Actual Class'] ==0) & (df['Predicted Class']==0)]\n",
    "tp0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true positive for class 1\n",
    "tp1=df[(df['Actual Class'] ==1) & (df['Predicted Class']==1)]\n",
    "tp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true positive for class 2\n",
    "tp2=df[(df['Actual Class'] ==2) & (df['Predicted Class']==2)]\n",
    "tp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true positive fro class 3\n",
    "tp3=df[(df['Actual Class'] ==3) & (df['Predicted Class']==3)]\n",
    "tp3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2)\n",
      "(4, 2)\n",
      "(4, 2)\n"
     ]
    }
   ],
   "source": [
    "# actual class 0 predictive as class 1,2,3\n",
    "tp01=df[(df['Actual Class'] ==0) & (df['Predicted Class']==1)]\n",
    "print(tp01.shape)\n",
    "tp02=df[(df['Actual Class'] ==0) & (df['Predicted Class']==2)]\n",
    "print(tp02.shape)\n",
    "tp03=df[(df['Actual Class'] ==0) & (df['Predicted Class']==3)]\n",
    "print(tp03.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "(3, 2)\n",
      "(0, 2)\n"
     ]
    }
   ],
   "source": [
    "# actual class 1 predictive as class 0,2,3\n",
    "tp10=df[(df['Actual Class'] ==1) & (df['Predicted Class']==0)]\n",
    "print(tp10.shape)\n",
    "tp12=df[(df['Actual Class'] ==1) & (df['Predicted Class']==2)]\n",
    "print(tp12.shape)\n",
    "tp13=df[(df['Actual Class'] ==1) & (df['Predicted Class']==3)]\n",
    "print(tp13.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(2, 2)\n",
      "(6, 2)\n"
     ]
    }
   ],
   "source": [
    "# actual class 2 predictive as class 0,1,3\n",
    "tp20=df[(df['Actual Class'] ==2) & (df['Predicted Class']==0)]\n",
    "print(tp20.shape)\n",
    "tp22=df[(df['Actual Class'] ==2) & (df['Predicted Class']==1)]\n",
    "print(tp22.shape)\n",
    "tp23=df[(df['Actual Class'] ==2) & (df['Predicted Class']==3)]\n",
    "print(tp23.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(3, 2)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "# actual class 3 predictive as class 0,1,2\n",
    "tp30=df[(df['Actual Class'] ==3) & (df['Predicted Class']==0)]\n",
    "print(tp30.shape)\n",
    "tp31=df[(df['Actual Class'] ==3) & (df['Predicted Class']==1)]\n",
    "print(tp31.shape)\n",
    "tp32=df[(df['Actual Class'] ==3) & (df['Predicted Class']==2)]\n",
    "print(tp32.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By using the above output of the cell, we can draw the confusion matrix.\n",
    "# 2.b\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "               predicted\n",
    "               0   1   2   3\n",
    "          \n",
    "           0   33  8   4   4\n",
    "           \n",
    "           1   1   32  3   0\n",
    " actual          \n",
    "           2   3   2   29  6\n",
    "           \n",
    "           3   2   3   3   27\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75625"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#overall accuracy\n",
    "overall_accuracy=(33+32+29+27)/160\n",
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673469387755102"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class 0 accuracy\n",
    "class0_accuracy=33/(33+8+4+4)\n",
    "class0_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 1 accuracy\n",
    "class1_accuracy=32/(1+32+3+0)\n",
    "class1_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.725"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 2 accuracy\n",
    "class2_accuracy=29/(3+2+29+6)\n",
    "class2_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7714285714285715"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class 3 accuracy\n",
    "class3_accuracy=27/(2+3+3+27)\n",
    "class3_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            predicted\n",
    "               0   1   2   3\n",
    "          \n",
    "           0   33  8   4   4\n",
    "           \n",
    "           1   1   32  3   0\n",
    " actual          \n",
    "           2   3   2   29  6\n",
    "           \n",
    "           3   2   3   3   27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp0=33\n",
    "tn0=32+3+2+3+29+6+3+27\n",
    "fp0=1+3+2\n",
    "fn0=8+4+4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we have all the necessary metrics for class 0 from the confusion matrix, now we can calculate the performance measures for class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8461538461538461\n",
      "0.673469387755102\n"
     ]
    }
   ],
   "source": [
    "# precision and recall of class 0\n",
    "precision0=tp0/(tp0+fp0)\n",
    "print(precision0)\n",
    "recall0=tp0/(tp0+fn0)\n",
    "print(recall0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              predicted\n",
    "               0   1   2   3\n",
    "          \n",
    "           0   33  8   4   4\n",
    "           \n",
    "           1   1   32  3   0\n",
    " actual          \n",
    "           2   3   2   29  6\n",
    "           \n",
    "           3   2   3   3   27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp1=32\n",
    "tn1=33+4+4+3+2+29+6+27+3\n",
    "fp1=8+2+3\n",
    "fn1=1+3+0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7111111111111111\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# precision and recall of class 1\n",
    "precision1=tp1/(tp1+fp1)\n",
    "print(precision1)\n",
    "recall1=tp1/(tp1+fn1)\n",
    "print(recall1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "              predicted\n",
    "               0   1   2   3\n",
    "          \n",
    "           0   33  8   4   4\n",
    "           \n",
    "           1   1   32  3   0\n",
    " actual          \n",
    "           2   3   2   29  6\n",
    "           \n",
    "           3   2   3   3   27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp2=29\n",
    "tn2=33+8+4+1+32+0+2+3+27\n",
    "fp2=4+3+3\n",
    "fn2=3+2+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7435897435897436\n",
      "0.725\n"
     ]
    }
   ],
   "source": [
    "# precision and recall of class 2\n",
    "precision2=tp2/(tp2+fp2)\n",
    "print(precision2)\n",
    "recall2=tp2/(tp2+fn2)\n",
    "print(recall2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              predicted\n",
    "               0   1   2   3\n",
    "          \n",
    "           0   33  8   4   4\n",
    "           \n",
    "           1   1   32  3   0\n",
    " actual          \n",
    "           2   3   2   29  6\n",
    "           \n",
    "           3   2   3   3   27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp3=27\n",
    "tn3=33+8+4+1+32+3+3+2+29\n",
    "fp3=4+0+6\n",
    "fn3=2+3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7297297297297297\n",
      "0.7714285714285715\n"
     ]
    }
   ],
   "source": [
    "# precision and recall of class 3\n",
    "precision3=tp3/(tp3+fp3)\n",
    "print(precision3)\n",
    "recall3=tp3/(tp3+fn3)\n",
    "print(recall3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macro-avg is mean average macro-avg is mean average precision/recall of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7576461076461076"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_avg_precision=(precision0+precision1+precision2+precision3)/4\n",
    "macro_avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7646967120181406"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_avg_recall=(recall0+recall1+recall2+recall3)/4\n",
    "macro_avg_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights for each class are the total number of samples of that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7649271472056283"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_avg_precision=(precision0*49+precision1*36+precision2*38+precision3*35)/(49+36+38+35)\n",
    "weighted_avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7566455696202532"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_avg_recall=(recall0*49+recall1*36+recall2*38+recall3*35)/(49+36+38+35)\n",
    "weighted_avg_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-1 Score\n",
    "# 2.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class0_f1=2*(precision0*recall0)/(precision0+recall0)\n",
    "class0_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7901234567901234"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1_f1=2*(precision1*recall1)/(precision1+recall1)\n",
    "class1_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7341772151898733"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2_f1=2*(precision2*recall2)/(precision2+recall2)\n",
    "class2_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class3_f1=2*(precision3*recall3)/(precision3+recall3)\n",
    "class3_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.405405405405405\n",
      "32.6530612244898\n"
     ]
    }
   ],
   "source": [
    "# type-1 and 2 error of class 0\n",
    "class0_type_1=fp0/(fp0+tn0)\n",
    "print(class0_type_1*100)\n",
    "class0_type_2=fn0/(fn0+tp0)\n",
    "print(class0_type_2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.483870967741936\n",
      "11.11111111111111\n"
     ]
    }
   ],
   "source": [
    "# type-1 and 2 error of class 1\n",
    "class1_type_1=fp1/(fp1+tn1)\n",
    "print(class1_type_1*100)\n",
    "class1_type_2=fn1/(fn1+tp1)\n",
    "print(class1_type_2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.333333333333332\n",
      "27.500000000000004\n"
     ]
    }
   ],
   "source": [
    "# type-1 and 2 error of class 2\n",
    "class2_type_1=fp2/(fp2+tn2)\n",
    "print(class2_type_1*100)\n",
    "class2_type_2=fn2/(fn2+tp2)\n",
    "print(class2_type_2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "22.857142857142858\n"
     ]
    }
   ],
   "source": [
    "# type-1 and 2 error of class 3\n",
    "class3_type_1=fp3/(fp3+tn3)\n",
    "print(class3_type_1*100)\n",
    "class3_type_2=fn3/(fn3+tp3)\n",
    "print(class3_type_2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[33  8  4  4]\n",
      " [ 1 32  3  0]\n",
      " [ 3  2 29  6]\n",
      " [ 2  3  3 27]]\n",
      "\n",
      "Accuracy: 0.76\n",
      "\n",
      "Macro Precision: 0.76\n",
      "Macro Recall: 0.76\n",
      "Macro F1-score: 0.76\n",
      "\n",
      "Weighted Precision: 0.76\n",
      "Weighted Recall: 0.76\n",
      "Weighted F1-score: 0.76\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.85      0.67      0.75        49\n",
      "     Class 1       0.71      0.89      0.79        36\n",
      "     Class 2       0.74      0.72      0.73        40\n",
      "     Class 3       0.73      0.77      0.75        35\n",
      "\n",
      "    accuracy                           0.76       160\n",
      "   macro avg       0.76      0.76      0.76       160\n",
      "weighted avg       0.76      0.76      0.76       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = df['Actual Class']\n",
    "y_pred = df['Predicted Class']\n",
    "\n",
    "\n",
    "#importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "#importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0','Class 1', 'Class 2', 'Class 3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision Table between sklearn output and standard output\n",
    "# 2.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Camparison Report                                                 \n",
    "\n",
    "\n",
    "\n",
    "              precision_sklearn/standard   recall_sklearn/standard  f1-score_sklearn/standard    \n",
    "\n",
    "     Class 0       0.85/.85                          0.67/.67                      0.75/.75        \n",
    "     Class 1       0.71/.71                          0.89/.89                      0.79/.79        \n",
    "     Class 2       0.74/.74                         0.72/.72                      0.73/.73        \n",
    "     Class 3       0.73/.73                         0.77/.77                      0.75/.75         \n",
    "\n",
    "    accuracy                           0.76/.75        \n",
    "   macro avg       0.76/.76      0.76/.76      0.76/.76        \n",
    "weighted avg       0.76/.76      0.76/.76      0.76/.76        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred contain different number of classes 4, 2. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [0 1 2 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-35ba81c3de23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#using log loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[0;32m   2269\u001b[0m                              \"y_true: {2}\".format(transformed_labels.shape[1],\n\u001b[0;32m   2270\u001b[0m                                                   \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2271\u001b[1;33m                                                   lb.classes_))\n\u001b[0m\u001b[0;32m   2272\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2273\u001b[0m             raise ValueError('The number of classes in labels is different '\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred contain different number of classes 4, 2. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [0 1 2 3]"
     ]
    }
   ],
   "source": [
    "#using log loss\n",
    "from sklearn.metrics import log_loss\n",
    "accuracy = log_loss(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refrences:1-https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-https://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
